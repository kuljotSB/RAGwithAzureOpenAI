{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8162e723",
   "metadata": {},
   "source": [
    "## Getting Started with Azure AI Content Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33599ed",
   "metadata": {},
   "source": [
    "![Azure AI Content Understanding](./Assets/content-understanding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f51da",
   "metadata": {},
   "source": [
    "%pip install python-dotenv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d90bc",
   "metadata": {},
   "source": [
    "### Setting up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c356a13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: https://carbonopsdevai4434735199.services.ai.azure.com\n",
      "API Key: 8QczczNUEkTbd9t0aBu0Eaxaq68GgHHuIhgcoGrL0CS9i0phIe19JQQJ99BFACfhMk5XJ3w3AAAAACOGJGL7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CONTENT_UNDERSTANDING_ENDPOINT = os.getenv(\"CONTENT_UNDERSTANDING_ENDPOINT\").strip().rstrip('/')\n",
    "CONTENT_UNDERSTANDING_API_KEY = os.getenv(\"CONTENT_UNDERSTANDING_API_KEY\")\n",
    "\n",
    "print(\"Endpoint:\", CONTENT_UNDERSTANDING_ENDPOINT)\n",
    "print(\"API Key:\", CONTENT_UNDERSTANDING_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0b173",
   "metadata": {},
   "source": [
    "### Using Prebuilt-Document Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80622884",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt_document_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-documentAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "document_url = \"https://github.com/kuljotSB/RAGwithAzureOpenAI/raw/refs/heads/main/ContentUnderstanding/Samples/invoice.pdf\"\n",
    "\n",
    "body = {\n",
    "    \"url\": document_url\n",
    "}\n",
    "\n",
    "document_analysis_result = {}\n",
    "\n",
    "try:\n",
    "    headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "            }\n",
    "\n",
    "    response = requests.post(prebuilt_document_analyzer_url, headers=headers, json=body)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    analysis_id = result.get(\"id\")\n",
    "    print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "    # Using the analysis ID to get results; polling until the analysis is complete\n",
    "    get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "    }\n",
    "    analysis_status = \"Running\"\n",
    "    while analysis_status == \"Running\":\n",
    "        status_response = requests.get(get_result_url, headers=headers)\n",
    "        status_response.raise_for_status()\n",
    "        status_result = status_response.json()\n",
    "        analysis_status = status_result.get(\"status\")\n",
    "        print(\"Current Analysis Status:\", analysis_status)\n",
    "        if analysis_status == \"Running\":\n",
    "            import time\n",
    "            time.sleep(1)  # Wait before polling again\n",
    "    result_response = requests.get(get_result_url, headers=headers)\n",
    "    result_response.raise_for_status()\n",
    "    document_analysis_result = result_response.json()\n",
    "    print(\"Document Analysis Result:\", document_analysis_result)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfadcbef",
   "metadata": {},
   "source": [
    "### Function to display Document Analyzer results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "def _hr(char=\"─\", width=80):\n",
    "    return char * width\n",
    "\n",
    "def _h(text: str, width=80):\n",
    "    pad = \" \" * 2\n",
    "    line = f\"{pad}{text.strip()} \"\n",
    "    return f\"{_hr('=')} \\n{line}\\n{_hr('=')}\"\n",
    "\n",
    "def _subh(text: str):\n",
    "    return f\"\\n{text}\\n{_hr()}\"\n",
    "\n",
    "def _kv(k: str, v: Any, k_width=22):\n",
    "    k = (k or \"\").strip()\n",
    "    if isinstance(v, (dict, list)):\n",
    "        v_str = json.dumps(v, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        v_str = \"\" if v is None else str(v)\n",
    "    return f\"{k:<{k_width}} : {v_str}\"\n",
    "\n",
    "def _wrap_block(text: str, width=100, indent=\"    \"):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    wrapped = textwrap.fill(text, width=width)\n",
    "    return textwrap.indent(wrapped, indent)\n",
    "\n",
    "def display_document_analyzer_content_understanding_result(\n",
    "    analysis_result: Dict[str, Any],\n",
    "    save_markdown_path: Optional[str] = None,\n",
    "    max_markdown_chars: int = 1200,\n",
    "    width: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-prints Azure Content Understanding 'prebuilt-documentAnalyzer' result\n",
    "    and optionally writes extracted Markdown to a file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    analysis_result : dict\n",
    "        The JSON-decoded response you printed as \"Analysis Result\".\n",
    "    save_markdown_path : str | None\n",
    "        If provided (e.g., 'analysis.md'), concatenated markdown from all contents\n",
    "        will be written to this path.\n",
    "    max_markdown_chars : int\n",
    "        Truncate console preview of markdown to this many characters (file is not truncated).\n",
    "    width : int\n",
    "        Wrap width for console output.\n",
    "    \"\"\"\n",
    "    # top-level\n",
    "    print(_h(\"Content Understanding • Analysis Summary\", width))\n",
    "    print(_kv(\"Analysis ID\", analysis_result.get(\"id\")))\n",
    "    print(_kv(\"Status\", analysis_result.get(\"status\")))\n",
    "\n",
    "    result = (analysis_result or {}).get(\"result\", {})\n",
    "    usage = (analysis_result or {}).get(\"usage\", {})\n",
    "    tokens = usage.get(\"tokens\", {}) if isinstance(usage, dict) else {}\n",
    "\n",
    "    print(_subh(\"Analyzer Info\"))\n",
    "    print(_kv(\"Analyzer ID\", result.get(\"analyzerId\")))\n",
    "    print(_kv(\"API Version\", result.get(\"apiVersion\")))\n",
    "    created_at = result.get(\"createdAt\")\n",
    "    try:\n",
    "        created_at_local = (\n",
    "            datetime.fromisoformat(created_at.replace(\"Z\", \"+00:00\")).astimezone().isoformat()\n",
    "            if created_at else None\n",
    "        )\n",
    "    except Exception:\n",
    "        created_at_local = created_at\n",
    "    print(_kv(\"Created At (UTC)\", created_at))\n",
    "    print(_kv(\"Created At (local)\", created_at_local))\n",
    "    warnings = result.get(\"warnings\") or []\n",
    "    print(_kv(\"Warnings\", f\"{len(warnings)}\"))\n",
    "\n",
    "    print(_subh(\"Usage\"))\n",
    "    # note: the API you showed returns floats for tokens; just print raw\n",
    "    for k in (\"contextualization\", \"input\", \"output\"):\n",
    "        if k in tokens:\n",
    "            print(_kv(f\"Tokens.{k}\", tokens.get(k)))\n",
    "\n",
    "    # contents\n",
    "    contents = result.get(\"contents\") or []\n",
    "    print(_subh(f\"Contents ({len(contents)})\"))\n",
    "\n",
    "    combined_md_parts = []\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        sp = item.get(\"startPageNumber\")\n",
    "        ep = item.get(\"endPageNumber\")\n",
    "        print(_hr())\n",
    "        print(f\"[Content #{idx}] kind={kind}  pages={sp}–{ep}\")\n",
    "\n",
    "        # Fields block (generic)\n",
    "        fields = (item.get(\"fields\") or {})\n",
    "        if fields:\n",
    "            print(\"• Fields:\")\n",
    "            for fname, fval in fields.items():\n",
    "                if isinstance(fval, dict):\n",
    "                    ftype = fval.get(\"type\")\n",
    "                    vstr = fval.get(\"valueString\") or fval.get(\"valueNumber\") or fval.get(\"valueBoolean\") or fval.get(\"valueArray\") or fval.get(\"valueObject\")\n",
    "                    # fall back to full dict if none of the canonical keys exist\n",
    "                    if vstr is None:\n",
    "                        vstr = fval\n",
    "                    print(_wrap_block(f\"  - {fname} ({ftype}): {vstr}\", width))\n",
    "                else:\n",
    "                    print(_wrap_block(f\"  - {fname}: {fval}\", width))\n",
    "\n",
    "        # Markdown preview\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "        combined_md_parts.append(md)\n",
    "        preview = md.strip()\n",
    "        preview_trunc = (preview[:max_markdown_chars] + \" … [truncated]\") if len(preview) > max_markdown_chars else preview\n",
    "        print(\"• Markdown Preview:\")\n",
    "        print(_wrap_block(preview_trunc, width))\n",
    "\n",
    "    # Save concatenated markdown if requested\n",
    "    if save_markdown_path:\n",
    "        all_md = \"\\n\\n---\\n\\n\".join(part for part in combined_md_parts if part)\n",
    "        out_path = Path(save_markdown_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        out_path.write_text(all_md, encoding=\"utf-8\")\n",
    "        print(_subh(\"Files\"))\n",
    "        print(_kv(\"Markdown saved\", str(out_path.resolve())))\n",
    "\n",
    "display_document_analyzer_content_understanding_result(document_analysis_result, save_markdown_path=\"document_analysis.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fb282",
   "metadata": {},
   "source": [
    "### Testing Image Analysis using Prebuilt-Image Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e98752",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt_image_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-imageAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "image_url = \"https://github.com/kuljotSB/RAGwithAzureOpenAI/raw/refs/heads/main/ContentUnderstanding/Samples/pieChart.png\"\n",
    "\n",
    "body = {\n",
    "    \"url\": image_url\n",
    "}\n",
    "\n",
    "image_analysis_result = {}\n",
    "\n",
    "try:\n",
    "    headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "            }\n",
    "\n",
    "    response = requests.post(prebuilt_image_analyzer_url, headers=headers, json=body)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    analysis_id = result.get(\"id\")\n",
    "    print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "    # Using the analysis ID to get results; polling until the analysis is complete\n",
    "    get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "    }\n",
    "    analysis_status = \"Running\"\n",
    "    while analysis_status == \"Running\":\n",
    "        status_response = requests.get(get_result_url, headers=headers)\n",
    "        status_response.raise_for_status()\n",
    "        status_result = status_response.json()\n",
    "        analysis_status = status_result.get(\"status\")\n",
    "        print(\"Current Analysis Status:\", analysis_status)\n",
    "        if analysis_status == \"Running\":\n",
    "            import time\n",
    "            time.sleep(1)  # Wait before polling again\n",
    "    result_response = requests.get(get_result_url, headers=headers)\n",
    "    result_response.raise_for_status()\n",
    "    image_analysis_result = result_response.json()\n",
    "    print(\"Image Analysis Result:\", image_analysis_result)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60327e",
   "metadata": {},
   "source": [
    "### Printing Image Analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e362be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "# ---------- small console helpers ----------\n",
    "def _hr(char=\"─\", width=80): return char * width\n",
    "def _h(text: str, width=80): return f\"{_hr('=')} \\n  {text.strip()} \\n{_hr('=')}\"\n",
    "def _subh(text: str): return f\"\\n{text}\\n{_hr()}\"\n",
    "def _kv(k: str, v: Any, k_width=22):\n",
    "    if isinstance(v, (dict, list)):\n",
    "        v_str = json.dumps(v, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        v_str = \"\" if v is None else str(v)\n",
    "    return f\"{k:<{k_width}} : {v_str}\"\n",
    "def _wrap_block(text: str, width=100, indent=\"    \"):\n",
    "    if not text: return \"\"\n",
    "    return textwrap.indent(textwrap.fill(text, width=width), indent)\n",
    "\n",
    "# ---------- field formatting ----------\n",
    "def _extract_value_from_fieldobj(fobj: Dict[str, Any]) -> Tuple[str, Any]:\n",
    "    \"\"\"\n",
    "    From an Azure field object like {\"type\":\"string\",\"valueString\":\"...\"}\n",
    "    return (type, value) using the first present among common value* keys.\n",
    "    \"\"\"\n",
    "    ftype = fobj.get(\"type\") or \"unknown\"\n",
    "    for key in (\"valueString\", \"valueNumber\", \"valueBoolean\", \"valueArray\", \"valueObject\"):\n",
    "        if key in fobj:\n",
    "            return ftype, fobj[key]\n",
    "    return ftype, fobj\n",
    "\n",
    "def _fields_to_markdown(fields: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render a fields dict to Markdown bullets with types and values.\n",
    "    Complex values are JSON in fenced blocks.\n",
    "    \"\"\"\n",
    "    if not fields:\n",
    "        return \"_No fields._\"\n",
    "    lines = []\n",
    "    for fname, fval in fields.items():\n",
    "        if isinstance(fval, dict) and \"type\" in fval:\n",
    "            ftype, v = _extract_value_from_fieldobj(fval)\n",
    "        else:\n",
    "            ftype, v = \"unknown\", fval\n",
    "        if isinstance(v, (dict, list)):\n",
    "            v_md = \"```json\\n\" + json.dumps(v, indent=2, ensure_ascii=False) + \"\\n```\"\n",
    "        else:\n",
    "            v_md = str(v) if v is not None else \"\"\n",
    "        lines.append(f\"- **{fname}** (_{ftype}_): {v_md}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- main function ----------\n",
    "def display_image_analysis_result(\n",
    "    analysis_result: Dict[str, Any],\n",
    "    save_markdown_path: Optional[str] = None,\n",
    "    max_markdown_chars: int = 800,\n",
    "    width: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-prints Azure 'prebuilt-imageAnalyzer' result and optionally writes a\n",
    "    Markdown report that includes BOTH the analyzer markdown AND a Fields section.\n",
    "    \"\"\"\n",
    "    # Console summary\n",
    "    print(_h(\"Content Understanding • Image Analysis Summary\", width))\n",
    "    print(_kv(\"Analysis ID\", analysis_result.get(\"id\")))\n",
    "    print(_kv(\"Status\", analysis_result.get(\"status\")))\n",
    "\n",
    "    result = (analysis_result or {}).get(\"result\", {})\n",
    "    usage = (analysis_result or {}).get(\"usage\", {})\n",
    "    tokens = usage.get(\"tokens\", {}) if isinstance(usage, dict) else {}\n",
    "\n",
    "    print(_subh(\"Analyzer Info\"))\n",
    "    print(_kv(\"Analyzer ID\", result.get(\"analyzerId\")))\n",
    "    print(_kv(\"API Version\", result.get(\"apiVersion\")))\n",
    "    created_at = result.get(\"createdAt\")\n",
    "    try:\n",
    "        created_at_local = (\n",
    "            datetime.fromisoformat(created_at.replace(\"Z\", \"+00:00\")).astimezone().isoformat()\n",
    "            if created_at else None\n",
    "        )\n",
    "    except Exception:\n",
    "        created_at_local = created_at\n",
    "    print(_kv(\"Created At (UTC)\", created_at))\n",
    "    print(_kv(\"Created At (local)\", created_at_local))\n",
    "    warnings = result.get(\"warnings\") or []\n",
    "    print(_kv(\"Warnings\", f\"{len(warnings)}\"))\n",
    "\n",
    "    print(_subh(\"Usage\"))\n",
    "    for k in (\"contextualization\", \"input\", \"output\"):\n",
    "        if k in tokens:\n",
    "            print(_kv(f\"Tokens.{k}\", tokens.get(k)))\n",
    "\n",
    "    contents = result.get(\"contents\") or []\n",
    "    print(_subh(f\"Contents ({len(contents)})\"))\n",
    "\n",
    "    # Prepare Markdown report parts (if requested)\n",
    "    md_parts = []\n",
    "    if save_markdown_path:\n",
    "        md_parts.append(\"# Image Analysis Report\\n\")\n",
    "        md_parts.append(\"## Summary\\n\")\n",
    "        md_parts.append(f\"- **Analysis ID:** `{analysis_result.get('id')}`\")\n",
    "        md_parts.append(f\"- **Status:** `{analysis_result.get('status')}`\")\n",
    "        md_parts.append(f\"- **Analyzer:** `{result.get('analyzerId')}`\")\n",
    "        md_parts.append(f\"- **API Version:** `{result.get('apiVersion')}`\")\n",
    "        md_parts.append(f\"- **Created At (UTC):** `{created_at}`\")\n",
    "        md_parts.append(f\"- **Warnings:** `{len(warnings)}`\\n\")\n",
    "\n",
    "        if tokens:\n",
    "            md_parts.append(\"## Usage\\n\")\n",
    "            for k in (\"contextualization\", \"input\", \"output\"):\n",
    "                if k in tokens:\n",
    "                    md_parts.append(f\"- **Tokens.{k}:** `{tokens.get(k)}`\")\n",
    "            md_parts.append(\"\")\n",
    "\n",
    "        md_parts.append(\"## Contents\\n\")\n",
    "\n",
    "    # Iterate content blocks\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        sp = item.get(\"startPageNumber\")\n",
    "        ep = item.get(\"endPageNumber\")\n",
    "        unit = item.get(\"unit\")\n",
    "        pages = item.get(\"pages\")\n",
    "        fields = (item.get(\"fields\") or {})\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "\n",
    "        # Console view\n",
    "        print(_hr())\n",
    "        print(f\"[Content #{idx}] kind={kind}  pages={sp}–{ep}\")\n",
    "        if unit:  print(_kv(\"Unit\", unit))\n",
    "        if pages: print(_kv(\"Pages\", pages))\n",
    "\n",
    "        if fields:\n",
    "            print(\"• Fields:\")\n",
    "            for fname, fval in fields.items():\n",
    "                if isinstance(fval, dict):\n",
    "                    ftype, v = _extract_value_from_fieldobj(fval)\n",
    "                    if v is None: v = fval\n",
    "                    print(_wrap_block(f\"  - {fname} ({ftype}): {v}\", width))\n",
    "                else:\n",
    "                    print(_wrap_block(f\"  - {fname}: {fval}\", width))\n",
    "\n",
    "        preview = md.strip()\n",
    "        preview_trunc = (preview[:max_markdown_chars] + \" … [truncated]\") if len(preview) > max_markdown_chars else preview\n",
    "        print(\"• Markdown Preview:\")\n",
    "        print(_wrap_block(preview_trunc, width))\n",
    "\n",
    "        # Markdown report for this content\n",
    "        if save_markdown_path:\n",
    "            md_parts.append(f\"### Content #{idx}\")\n",
    "            meta_bits = []\n",
    "            if kind: meta_bits.append(f\"**kind:** `{kind}`\")\n",
    "            if sp is not None and ep is not None: meta_bits.append(f\"**pages:** `{sp}–{ep}`\")\n",
    "            if unit: meta_bits.append(f\"**unit:** `{unit}`\")\n",
    "            if meta_bits:\n",
    "                md_parts.append(\"> \" + \" • \".join(meta_bits))\n",
    "            if pages:\n",
    "                md_parts.append(\"<details><summary>Pages metadata</summary>\\n\\n```json\\n\" +\n",
    "                                json.dumps(pages, indent=2, ensure_ascii=False) +\n",
    "                                \"\\n```\\n</details>\\n\")\n",
    "            # Analyzer-provided markdown (exact)\n",
    "            md_parts.append(\"#### Analyzer Markdown\")\n",
    "            md_parts.append(md if md.strip() else \"_(empty)_\")\n",
    "            # Fields as Markdown bullets\n",
    "            md_parts.append(\"#### Fields\")\n",
    "            md_parts.append(_fields_to_markdown(fields))\n",
    "            md_parts.append(\"---\")\n",
    "\n",
    "    # Write Markdown file if requested\n",
    "    if save_markdown_path:\n",
    "        out_path = Path(save_markdown_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        report = \"\\n\".join(md_parts).rstrip() + \"\\n\"\n",
    "        out_path.write_text(report, encoding=\"utf-8\")\n",
    "        print(_subh(\"Files\"))\n",
    "        print(_kv(\"Markdown saved\", str(out_path.resolve())))\n",
    "\n",
    "display_image_analysis_result(image_analysis_result, save_markdown_path=\"image_analysis.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a786e1",
   "metadata": {},
   "source": [
    "### Testing Audio Analysis using Prebuilt-Audio Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b988db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt_audio_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-audioAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "audio_url = \"https://github.com/kuljotSB/RAGwithAzureOpenAI/raw/refs/heads/main/ContentUnderstanding/Samples/audio.wav\"\n",
    "\n",
    "body = {\n",
    "    \"url\": audio_url\n",
    "}\n",
    "\n",
    "audio_analysis_result = {}\n",
    "\n",
    "try:\n",
    "    headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "            }\n",
    "\n",
    "    response = requests.post(prebuilt_audio_analyzer_url, headers=headers, json=body)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    analysis_id = result.get(\"id\")\n",
    "    print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "    # Using the analysis ID to get results; polling until the analysis is complete\n",
    "    get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "    }\n",
    "    analysis_status = \"Running\"\n",
    "    while analysis_status == \"Running\":\n",
    "        status_response = requests.get(get_result_url, headers=headers)\n",
    "        status_response.raise_for_status()\n",
    "        status_result = status_response.json()\n",
    "        analysis_status = status_result.get(\"status\")\n",
    "        print(\"Current Analysis Status:\", analysis_status)\n",
    "        if analysis_status == \"Running\":\n",
    "            import time\n",
    "            time.sleep(3)  # Wait before polling again\n",
    "    result_response = requests.get(get_result_url, headers=headers)\n",
    "    result_response.raise_for_status()\n",
    "    audio_analysis_result = result_response.json()\n",
    "    print(\"Audio Analysis Result:\", audio_analysis_result)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31521898",
   "metadata": {},
   "source": [
    "### Printing Audio Analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4333f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "\n",
    "# ---------- small console helpers ----------\n",
    "def _hr(char=\"─\", width=80): return char * width\n",
    "def _h(text: str, width=80): return f\"{_hr('=')} \\n  {text.strip()} \\n{_hr('=')}\"\n",
    "def _subh(text: str): return f\"\\n{text}\\n{_hr()}\"\n",
    "def _kv(k: str, v: Any, k_width=24):\n",
    "    if isinstance(v, (dict, list)):\n",
    "        v_str = json.dumps(v, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        v_str = \"\" if v is None else str(v)\n",
    "    return f\"{k:<{k_width}} : {v_str}\"\n",
    "def _wrap_block(text: str, width=100, indent=\"    \"):\n",
    "    if not text: return \"\"\n",
    "    return textwrap.indent(textwrap.fill(text, width=width), indent)\n",
    "\n",
    "# ---------- field formatting ----------\n",
    "def _extract_value_from_fieldobj(fobj: Dict[str, Any]) -> Tuple[str, Any]:\n",
    "    \"\"\"\n",
    "    From an Azure 'field' object like {\"type\":\"string\",\"valueString\":\"...\"}\n",
    "    return (type, value) using the first present among value* keys.\n",
    "    \"\"\"\n",
    "    ftype = fobj.get(\"type\") or \"unknown\"\n",
    "    for key in (\"valueString\", \"valueNumber\", \"valueBoolean\", \"valueArray\", \"valueObject\"):\n",
    "        if key in fobj:\n",
    "            return ftype, fobj[key]\n",
    "    return ftype, fobj\n",
    "\n",
    "def _fields_to_markdown(fields: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render a fields dict to Markdown bullets with types and values.\n",
    "    Complex values are JSON in fenced blocks.\n",
    "    \"\"\"\n",
    "    if not fields:\n",
    "        return \"_No fields._\"\n",
    "    lines = []\n",
    "    for fname, fval in fields.items():\n",
    "        if isinstance(fval, dict) and \"type\" in fval:\n",
    "            ftype, v = _extract_value_from_fieldobj(fval)\n",
    "        else:\n",
    "            ftype, v = \"unknown\", fval\n",
    "        if isinstance(v, (dict, list)):\n",
    "            v_md = \"```json\\n\" + json.dumps(v, indent=2, ensure_ascii=False) + \"\\n```\"\n",
    "        else:\n",
    "            v_md = str(v) if v is not None else \"\"\n",
    "        lines.append(f\"- **{fname}** (_{ftype}_): {v_md}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- audio utilities ----------\n",
    "def _fmt_ms(ms: int) -> str:\n",
    "    \"\"\"Format milliseconds as HH:MM:SS.mmm\"\"\"\n",
    "    if ms is None:\n",
    "        return \"\"\n",
    "    td = timedelta(milliseconds=int(ms))\n",
    "    # Manually format to always show milliseconds\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    seconds = total_seconds % 60\n",
    "    millis = int(ms) % 1000\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{millis:03d}\"\n",
    "\n",
    "def _phrases_to_markdown(phrases: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Render transcriptPhrases into a compact Markdown list with timestamps, speaker, confidence, and text.\n",
    "    \"\"\"\n",
    "    if not phrases:\n",
    "        return \"_No transcript phrases._\"\n",
    "    out = []\n",
    "    for p in phrases:\n",
    "        spk = p.get(\"speaker\") or \"Speaker\"\n",
    "        st = _fmt_ms(p.get(\"startTimeMs\"))\n",
    "        et = _fmt_ms(p.get(\"endTimeMs\"))\n",
    "        conf = p.get(\"confidence\")\n",
    "        txt = p.get(\"text\") or \"\"\n",
    "        conf_str = f\"{conf:.3f}\" if isinstance(conf, (int, float)) else str(conf) if conf is not None else \"\"\n",
    "        out.append(f\"- `{st} → {et}` **{spk}** (_conf: {conf_str}_): {txt}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "# ---------- main function ----------\n",
    "def display_audio_analysis_result(\n",
    "    analysis_result: Dict[str, Any],\n",
    "    save_markdown_path: Optional[str] = None,\n",
    "    max_markdown_chars: int = 1600,\n",
    "    width: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-prints Azure 'prebuilt-audioAnalyzer' result and optionally writes a\n",
    "    Markdown report that includes BOTH the analyzer markdown AND a Fields section,\n",
    "    plus a compact transcript phrases section.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    analysis_result : dict\n",
    "        The JSON-decoded response (\"Audio Analysis Result\").\n",
    "    save_markdown_path : str | None\n",
    "        If provided, writes a Markdown report with analyzer markdown, fields, and phrases.\n",
    "    max_markdown_chars : int\n",
    "        Truncate console preview of analyzer markdown to this many characters.\n",
    "    width : int\n",
    "        Wrap width for console output.\n",
    "    \"\"\"\n",
    "    # Console summary\n",
    "    print(_h(\"Content Understanding • Audio Analysis Summary\", width))\n",
    "    print(_kv(\"Analysis ID\", analysis_result.get(\"id\")))\n",
    "    print(_kv(\"Status\", analysis_result.get(\"status\")))\n",
    "\n",
    "    result = (analysis_result or {}).get(\"result\", {})\n",
    "    usage = (analysis_result or {}).get(\"usage\", {}) or {}\n",
    "    tokens = usage.get(\"tokens\", {}) if isinstance(usage, dict) else {}\n",
    "    audio_hours = usage.get(\"audioHours\")\n",
    "\n",
    "    print(_subh(\"Analyzer Info\"))\n",
    "    print(_kv(\"Analyzer ID\", result.get(\"analyzerId\")))\n",
    "    print(_kv(\"API Version\", result.get(\"apiVersion\")))\n",
    "    print(_kv(\"String Encoding\", result.get(\"stringEncoding\")))\n",
    "    created_at = result.get(\"createdAt\")\n",
    "    try:\n",
    "        created_at_local = (\n",
    "            datetime.fromisoformat(created_at.replace(\"Z\", \"+00:00\")).astimezone().isoformat()\n",
    "            if created_at else None\n",
    "        )\n",
    "    except Exception:\n",
    "        created_at_local = created_at\n",
    "    print(_kv(\"Created At (UTC)\", created_at))\n",
    "    print(_kv(\"Created At (local)\", created_at_local))\n",
    "    warnings = result.get(\"warnings\") or []\n",
    "    print(_kv(\"Warnings\", f\"{len(warnings)}\"))\n",
    "\n",
    "    print(_subh(\"Usage\"))\n",
    "    if audio_hours is not None:\n",
    "        print(_kv(\"Audio Hours\", audio_hours))\n",
    "    for k in (\"contextualization\", \"input\", \"output\"):\n",
    "        if k in tokens:\n",
    "            print(_kv(f\"Tokens.{k}\", tokens.get(k)))\n",
    "\n",
    "    contents = result.get(\"contents\") or []\n",
    "    print(_subh(f\"Contents ({len(contents)})\"))\n",
    "\n",
    "    # Prepare Markdown report parts (if requested)\n",
    "    md_parts = []\n",
    "    if save_markdown_path:\n",
    "        md_parts.append(\"# Audio Analysis Report\\n\")\n",
    "        md_parts.append(\"## Summary\\n\")\n",
    "        md_parts.append(f\"- **Analysis ID:** `{analysis_result.get('id')}`\")\n",
    "        md_parts.append(f\"- **Status:** `{analysis_result.get('status')}`\")\n",
    "        md_parts.append(f\"- **Analyzer:** `{result.get('analyzerId')}`\")\n",
    "        md_parts.append(f\"- **API Version:** `{result.get('apiVersion')}`\")\n",
    "        md_parts.append(f\"- **String Encoding:** `{result.get('stringEncoding')}`\")\n",
    "        md_parts.append(f\"- **Created At (UTC):** `{created_at}`\")\n",
    "        md_parts.append(f\"- **Warnings:** `{len(warnings)}`\\n\")\n",
    "        if audio_hours is not None:\n",
    "            md_parts.append(f\"- **Audio Hours:** `{audio_hours}`\")\n",
    "        if tokens:\n",
    "            md_parts.append(\"\\n## Usage\")\n",
    "            for k in (\"contextualization\", \"input\", \"output\"):\n",
    "                if k in tokens:\n",
    "                    md_parts.append(f\"- **Tokens.{k}:** `{tokens.get(k)}`\")\n",
    "        md_parts.append(\"\\n## Contents\\n\")\n",
    "\n",
    "    # Iterate content blocks\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        start_ms = item.get(\"startTimeMs\")\n",
    "        end_ms = item.get(\"endTimeMs\")\n",
    "        fields = (item.get(\"fields\") or {})\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "        phrases = item.get(\"transcriptPhrases\") or []\n",
    "\n",
    "        # Console view\n",
    "        print(_hr())\n",
    "        print(f\"[Content #{idx}] kind={kind}  time={_fmt_ms(start_ms)}–{_fmt_ms(end_ms)}\")\n",
    "        if fields:\n",
    "            print(\"• Fields:\")\n",
    "            for fname, fval in fields.items():\n",
    "                if isinstance(fval, dict):\n",
    "                    ftype, v = _extract_value_from_fieldobj(fval)\n",
    "                    if v is None: v = fval\n",
    "                    print(_wrap_block(f\"  - {fname} ({ftype}): {v}\", width))\n",
    "                else:\n",
    "                    print(_wrap_block(f\"  - {fname}: {fval}\", width))\n",
    "\n",
    "        preview = md.strip()\n",
    "        preview_trunc = (preview[:max_markdown_chars] + \" … [truncated]\") if len(preview) > max_markdown_chars else preview\n",
    "        print(\"• Analyzer Markdown Preview:\")\n",
    "        print(_wrap_block(preview_trunc, width))\n",
    "\n",
    "        if phrases:\n",
    "            print(\"• Transcript Phrases (compact):\")\n",
    "            print(_wrap_block(_phrases_to_markdown(phrases), width))\n",
    "\n",
    "        # Markdown report for this content\n",
    "        if save_markdown_path:\n",
    "            md_parts.append(f\"### Content #{idx}\")\n",
    "            meta_bits = []\n",
    "            if kind: meta_bits.append(f\"**kind:** `{kind}`\")\n",
    "            if start_ms is not None and end_ms is not None:\n",
    "                meta_bits.append(f\"**time:** `{_fmt_ms(start_ms)}–{_fmt_ms(end_ms)}`\")\n",
    "            if meta_bits:\n",
    "                md_parts.append(\"> \" + \" • \".join(meta_bits))\n",
    "\n",
    "            # Analyzer-provided markdown (exact)\n",
    "            md_parts.append(\"#### Analyzer Markdown\")\n",
    "            md_parts.append(md if md.strip() else \"_(empty)_\")\n",
    "\n",
    "            # Fields\n",
    "            md_parts.append(\"#### Fields\")\n",
    "            md_parts.append(_fields_to_markdown(fields))\n",
    "\n",
    "            # Transcript phrases (compact bullets)\n",
    "            md_parts.append(\"#### Transcript Phrases\")\n",
    "            md_parts.append(_phrases_to_markdown(phrases))\n",
    "\n",
    "            md_parts.append(\"---\")\n",
    "\n",
    "    # Write Markdown file if requested\n",
    "    if save_markdown_path:\n",
    "        out_path = Path(save_markdown_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        report = \"\\n\".join(md_parts).rstrip() + \"\\n\"\n",
    "        out_path.write_text(report, encoding=\"utf-8\")\n",
    "        print(_subh(\"Files\"))\n",
    "        print(_kv(\"Markdown saved\", str(out_path.resolve())))\n",
    "\n",
    "display_audio_analysis_result(audio_analysis_result, save_markdown_path=\"audio_analysis.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552abc4",
   "metadata": {},
   "source": [
    "### Testing Video Analysis using Prebuilt-Video Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa62da45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis ID: 9e1b42db-f197-4277-9f76-c54a392c0164\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Succeeded\n",
      "Video Analysis Result: {'id': '9e1b42db-f197-4277-9f76-c54a392c0164', 'status': 'Succeeded', 'result': {'analyzerId': 'prebuilt-videoAnalyzer', 'apiVersion': '2025-05-01-preview', 'createdAt': '2025-09-21T18:19:14Z', 'stringEncoding': 'utf8', 'warnings': [], 'contents': [{'markdown': \"# Video: 00:00.000 => 00:43.866\\nWidth: 1080\\nHeight: 608\\n\\n## Segment 1: 00:00.726 => 00:01.360\\nThe video begins with a visual of a flight simulator and Microsoft Azure AI logo, indicating a collaboration between the two entities.\\n\\nTranscript\\n```\\nWEBVTT\\n\\n00:01.360 --> 00:06.640\\n<Speaker 1>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\\n```\\n\\nKey Frames\\n- 00:00.726 ![](keyFrame.726.jpg)\\n\\n## Segment 2: 00:01.360 => 00:13.320\\nA speaker discusses the importance of good data for neural TTS, mentioning the creation of a universal TTS model based on 3,000 hours of data to capture audio nuances and generate natural voices.\\n\\nTranscript\\n```\\nWEBVTT\\n\\n00:01.360 --> 00:06.640\\n<Speaker 1>When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\\n\\n00:07.120 --> 00:13.320\\n<Speaker 2>To achieve that, we build a universal TTS model based on 3,000 hours of data.\\n```\\n\\nKey Frames\\n- 00:02.046 ![](keyFrame.2046.jpg)\\n- 00:02.640 ![](keyFrame.2640.jpg)\\n- 00:04.059 ![](keyFrame.4059.jpg)\\n- 00:04.884 ![](keyFrame.4884.jpg)\\n- 00:05.709 ![](keyFrame.5709.jpg)\\n- 00:06.534 ![](keyFrame.6534.jpg)\\n- 00:07.788 ![](keyFrame.7788.jpg)\\n- 00:08.976 ![](keyFrame.8976.jpg)\\n- 00:09.768 ![](keyFrame.9768.jpg)\\n- 00:10.560 ![](keyFrame.10560.jpg)\\n- 00:12.078 ![](keyFrame.12078.jpg)\\n- 00:12.804 ![](keyFrame.12804.jpg)\\n\\n## Segment 3: 00:13.320 => 00:23.680\\nThe video shows visuals of data centers and flight simulations, highlighting the accumulation of data and the high fidelity of cognitive services offerings that sound more like human voices.\\n\\nTranscript\\n```\\nWEBVTT\\n\\n00:07.120 --> 00:13.320\\n<Speaker 2>To achieve that, we build a universal TTS model based on 3,000 hours of data.\\n\\n00:13.440 --> 00:23.680\\n<Speaker 1>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\\n```\\n\\nKey Frames\\n- 00:14.190 ![](keyFrame.14190.jpg)\\n- 00:14.817 ![](keyFrame.14817.jpg)\\n- 00:15.444 ![](keyFrame.15444.jpg)\\n- 00:16.929 ![](keyFrame.16929.jpg)\\n- 00:17.754 ![](keyFrame.17754.jpg)\\n- 00:18.579 ![](keyFrame.18579.jpg)\\n- 00:20.196 ![](keyFrame.20196.jpg)\\n- 00:20.955 ![](keyFrame.20955.jpg)\\n- 00:21.714 ![](keyFrame.21714.jpg)\\n- 00:22.473 ![](keyFrame.22473.jpg)\\n- 00:23.232 ![](keyFrame.23232.jpg)\\n\\n## Segment 4: 00:23.680 => 00:41.283\\nThe video transitions to a demonstration of aircraft operations, including scenes of aircraft on the runway and communication between ground control and pilots.\\n\\nTranscript\\n```\\nWEBVTT\\n\\n00:13.440 --> 00:23.680\\n<Speaker 1>We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.\\n\\n00:24.040 --> 00:29.120\\n<Speaker 3>What we liked about cognitive services offerings were that they had a much higher fidelity.\\n\\n00:29.600 --> 00:32.880\\n<Speaker 3>And they sounded a lot more like an actual human voice.\\n\\n00:33.680 --> 00:37.200\\n<Speaker 4>Orlando ground 9555 requesting the end of pushback.\\n\\n00:38.680 --> 00:41.280\\n<Speaker 4>9555 request to end pushback received.\\n```\\n\\nKey Frames\\n- 00:24.816 ![](keyFrame.24816.jpg)\\n- 00:25.674 ![](keyFrame.25674.jpg)\\n- 00:26.532 ![](keyFrame.26532.jpg)\\n- 00:27.390 ![](keyFrame.27390.jpg)\\n- 00:28.248 ![](keyFrame.28248.jpg)\\n- 00:29.106 ![](keyFrame.29106.jpg)\\n- 00:30.822 ![](keyFrame.30822.jpg)\\n- 00:31.614 ![](keyFrame.31614.jpg)\\n- 00:32.406 ![](keyFrame.32406.jpg)\\n- 00:33.891 ![](keyFrame.33891.jpg)\\n- 00:34.584 ![](keyFrame.34584.jpg)\\n- 00:36.069 ![](keyFrame.36069.jpg)\\n- 00:36.861 ![](keyFrame.36861.jpg)\\n- 00:38.181 ![](keyFrame.38181.jpg)\\n- 00:38.676 ![](keyFrame.38676.jpg)\\n- 00:39.897 ![](keyFrame.39897.jpg)\\n- 00:40.590 ![](keyFrame.40590.jpg)\\n- 00:41.283 ![](keyFrame.41283.jpg)\\n\\n## Segment 5: 00:41.283 => 00:43.230\\nThe video concludes with the Microsoft logo, emphasizing the branding of the collaboration.\\n\\n\\n\\nKey Frames\\n- 00:41.283 ![](keyFrame.41283.jpg)\\n- 00:42.636 ![](keyFrame.42636.jpg)\\n- 00:43.230 ![](keyFrame.43230.jpg)\", 'fields': {'Segments': {'type': 'array', 'valueArray': [{'type': 'object', 'valueObject': {'SegmentId': {'type': 'string', 'valueString': '1'}, 'StartTimeMs': {'type': 'integer', 'valueInteger': 726}, 'EndTimeMs': {'type': 'integer', 'valueInteger': 1360}, 'Description': {'type': 'string', 'valueString': 'The video begins with a visual of a flight simulator and Microsoft Azure AI logo, indicating a collaboration between the two entities.'}}}, {'type': 'object', 'valueObject': {'SegmentId': {'type': 'string', 'valueString': '2'}, 'StartTimeMs': {'type': 'integer', 'valueInteger': 1360}, 'EndTimeMs': {'type': 'integer', 'valueInteger': 13320}, 'Description': {'type': 'string', 'valueString': 'A speaker discusses the importance of good data for neural TTS, mentioning the creation of a universal TTS model based on 3,000 hours of data to capture audio nuances and generate natural voices.'}}}, {'type': 'object', 'valueObject': {'SegmentId': {'type': 'string', 'valueString': '3'}, 'StartTimeMs': {'type': 'integer', 'valueInteger': 13320}, 'EndTimeMs': {'type': 'integer', 'valueInteger': 23680}, 'Description': {'type': 'string', 'valueString': 'The video shows visuals of data centers and flight simulations, highlighting the accumulation of data and the high fidelity of cognitive services offerings that sound more like human voices.'}}}, {'type': 'object', 'valueObject': {'SegmentId': {'type': 'string', 'valueString': '4'}, 'StartTimeMs': {'type': 'integer', 'valueInteger': 23680}, 'EndTimeMs': {'type': 'integer', 'valueInteger': 41283}, 'Description': {'type': 'string', 'valueString': 'The video transitions to a demonstration of aircraft operations, including scenes of aircraft on the runway and communication between ground control and pilots.'}}}, {'type': 'object', 'valueObject': {'SegmentId': {'type': 'string', 'valueString': '5'}, 'StartTimeMs': {'type': 'integer', 'valueInteger': 41283}, 'EndTimeMs': {'type': 'integer', 'valueInteger': 43230}, 'Description': {'type': 'string', 'valueString': 'The video concludes with the Microsoft logo, emphasizing the branding of the collaboration.'}}}]}}, 'kind': 'audioVisual', 'startTimeMs': 0, 'endTimeMs': 43866, 'width': 1080, 'height': 608, 'KeyFrameTimesMs': [726, 2046, 2640, 4059, 4884, 5709, 6534, 7788, 8976, 9768, 10560, 12078, 12804, 14190, 14817, 15444, 16929, 17754, 18579, 20196, 20955, 21714, 22473, 23232, 24816, 25674, 26532, 27390, 28248, 29106, 30822, 31614, 32406, 33891, 34584, 36069, 36861, 38181, 38676, 39897, 40590, 41283, 42636, 43230], 'transcriptPhrases': [{'speaker': 'Speaker 1', 'startTimeMs': 1360, 'endTimeMs': 6640, 'text': \"When it comes to the neural TTS, in order to get a good voice, it's better to have good data.\", 'confidence': 0.937, 'words': [{'startTimeMs': 1360, 'endTimeMs': 1600, 'text': 'When'}, {'startTimeMs': 1600, 'endTimeMs': 1760, 'text': 'it'}, {'startTimeMs': 1760, 'endTimeMs': 2000, 'text': 'comes'}, {'startTimeMs': 2000, 'endTimeMs': 2120, 'text': 'to'}, {'startTimeMs': 2120, 'endTimeMs': 2200, 'text': 'the'}, {'startTimeMs': 2200, 'endTimeMs': 2560, 'text': 'neural'}, {'startTimeMs': 2560, 'endTimeMs': 3200, 'text': 'TTS,'}, {'startTimeMs': 3200, 'endTimeMs': 3440, 'text': 'in'}, {'startTimeMs': 3440, 'endTimeMs': 3680, 'text': 'order'}, {'startTimeMs': 3680, 'endTimeMs': 3880, 'text': 'to'}, {'startTimeMs': 3960, 'endTimeMs': 4160, 'text': 'get'}, {'startTimeMs': 4160, 'endTimeMs': 4240, 'text': 'a'}, {'startTimeMs': 4240, 'endTimeMs': 4480, 'text': 'good'}, {'startTimeMs': 4480, 'endTimeMs': 4880, 'text': 'voice,'}, {'startTimeMs': 4960, 'endTimeMs': 5200, 'text': \"it's\"}, {'startTimeMs': 5200, 'endTimeMs': 5600, 'text': 'better'}, {'startTimeMs': 5600, 'endTimeMs': 5760, 'text': 'to'}, {'startTimeMs': 5760, 'endTimeMs': 6000, 'text': 'have'}, {'startTimeMs': 6000, 'endTimeMs': 6200, 'text': 'good'}, {'startTimeMs': 6200, 'endTimeMs': 6640, 'text': 'data.'}], 'locale': 'en-US'}, {'speaker': 'Speaker 2', 'startTimeMs': 7120, 'endTimeMs': 13320, 'text': 'To achieve that, we build a universal TTS model based on 3,000 hours of data.', 'confidence': 0.937, 'words': [{'startTimeMs': 7120, 'endTimeMs': 7360, 'text': 'To'}, {'startTimeMs': 7560, 'endTimeMs': 7880, 'text': 'achieve'}, {'startTimeMs': 7880, 'endTimeMs': 8160, 'text': 'that,'}, {'startTimeMs': 8160, 'endTimeMs': 8280, 'text': 'we'}, {'startTimeMs': 8320, 'endTimeMs': 8560, 'text': 'build'}, {'startTimeMs': 8560, 'endTimeMs': 8600, 'text': 'a'}, {'startTimeMs': 8720, 'endTimeMs': 9320, 'text': 'universal'}, {'startTimeMs': 9360, 'endTimeMs': 9760, 'text': 'TTS'}, {'startTimeMs': 9760, 'endTimeMs': 10240, 'text': 'model'}, {'startTimeMs': 10640, 'endTimeMs': 10840, 'text': 'based'}, {'startTimeMs': 11040, 'endTimeMs': 11200, 'text': 'on'}, {'startTimeMs': 11480, 'endTimeMs': 12160, 'text': '3,000'}, {'startTimeMs': 12320, 'endTimeMs': 12480, 'text': 'hours'}, {'startTimeMs': 12760, 'endTimeMs': 12880, 'text': 'of'}, {'startTimeMs': 12920, 'endTimeMs': 13320, 'text': 'data.'}], 'locale': 'en-US'}, {'speaker': 'Speaker 1', 'startTimeMs': 13440, 'endTimeMs': 23680, 'text': 'We actually accumulated tons of the data so that this universal model is able to capture the nuance of the audio and generate a more natural voice for the algorithm.', 'confidence': 0.937, 'words': [{'startTimeMs': 13440, 'endTimeMs': 13600, 'text': 'We'}, {'startTimeMs': 13600, 'endTimeMs': 14000, 'text': 'actually'}, {'startTimeMs': 14000, 'endTimeMs': 14720, 'text': 'accumulated'}, {'startTimeMs': 14720, 'endTimeMs': 15200, 'text': 'tons'}, {'startTimeMs': 15200, 'endTimeMs': 15360, 'text': 'of'}, {'startTimeMs': 15360, 'endTimeMs': 15480, 'text': 'the'}, {'startTimeMs': 15600, 'endTimeMs': 16080, 'text': 'data'}, {'startTimeMs': 16080, 'endTimeMs': 16240, 'text': 'so'}, {'startTimeMs': 16240, 'endTimeMs': 16440, 'text': 'that'}, {'startTimeMs': 16440, 'endTimeMs': 16640, 'text': 'this'}, {'startTimeMs': 16640, 'endTimeMs': 17120, 'text': 'universal'}, {'startTimeMs': 17120, 'endTimeMs': 17520, 'text': 'model'}, {'startTimeMs': 17600, 'endTimeMs': 17720, 'text': 'is'}, {'startTimeMs': 17760, 'endTimeMs': 18080, 'text': 'able'}, {'startTimeMs': 18080, 'endTimeMs': 18320, 'text': 'to'}, {'startTimeMs': 18360, 'endTimeMs': 18760, 'text': 'capture'}, {'startTimeMs': 18760, 'endTimeMs': 18880, 'text': 'the'}, {'startTimeMs': 18880, 'endTimeMs': 19440, 'text': 'nuance'}, {'startTimeMs': 19440, 'endTimeMs': 19520, 'text': 'of'}, {'startTimeMs': 19520, 'endTimeMs': 19680, 'text': 'the'}, {'startTimeMs': 19680, 'endTimeMs': 20240, 'text': 'audio'}, {'startTimeMs': 20400, 'endTimeMs': 20720, 'text': 'and'}, {'startTimeMs': 20720, 'endTimeMs': 21360, 'text': 'generate'}, {'startTimeMs': 21440, 'endTimeMs': 21520, 'text': 'a'}, {'startTimeMs': 21520, 'endTimeMs': 21760, 'text': 'more'}, {'startTimeMs': 21760, 'endTimeMs': 22280, 'text': 'natural'}, {'startTimeMs': 22280, 'endTimeMs': 22560, 'text': 'voice'}, {'startTimeMs': 22720, 'endTimeMs': 22960, 'text': 'for'}, {'startTimeMs': 22960, 'endTimeMs': 23080, 'text': 'the'}, {'startTimeMs': 23080, 'endTimeMs': 23680, 'text': 'algorithm.'}], 'locale': 'en-US'}, {'speaker': 'Speaker 3', 'startTimeMs': 24040, 'endTimeMs': 29120, 'text': 'What we liked about cognitive services offerings were that they had a much higher fidelity.', 'confidence': 0.937, 'words': [{'startTimeMs': 24040, 'endTimeMs': 24240, 'text': 'What'}, {'startTimeMs': 24240, 'endTimeMs': 24320, 'text': 'we'}, {'startTimeMs': 24320, 'endTimeMs': 24560, 'text': 'liked'}, {'startTimeMs': 24560, 'endTimeMs': 24880, 'text': 'about'}, {'startTimeMs': 24960, 'endTimeMs': 25440, 'text': 'cognitive'}, {'startTimeMs': 25440, 'endTimeMs': 25920, 'text': 'services'}, {'startTimeMs': 25920, 'endTimeMs': 26400, 'text': 'offerings'}, {'startTimeMs': 26480, 'endTimeMs': 26720, 'text': 'were'}, {'startTimeMs': 26720, 'endTimeMs': 27040, 'text': 'that'}, {'startTimeMs': 27200, 'endTimeMs': 27600, 'text': 'they'}, {'startTimeMs': 27600, 'endTimeMs': 27800, 'text': 'had'}, {'startTimeMs': 27800, 'endTimeMs': 27840, 'text': 'a'}, {'startTimeMs': 27840, 'endTimeMs': 28160, 'text': 'much'}, {'startTimeMs': 28160, 'endTimeMs': 28560, 'text': 'higher'}, {'startTimeMs': 28560, 'endTimeMs': 29120, 'text': 'fidelity.'}], 'locale': 'en-US'}, {'speaker': 'Speaker 3', 'startTimeMs': 29600, 'endTimeMs': 32880, 'text': 'And they sounded a lot more like an actual human voice.', 'confidence': 0.823, 'words': [{'startTimeMs': 29600, 'endTimeMs': 30080, 'text': 'And'}, {'startTimeMs': 30080, 'endTimeMs': 30160, 'text': 'they'}, {'startTimeMs': 30160, 'endTimeMs': 30480, 'text': 'sounded'}, {'startTimeMs': 30480, 'endTimeMs': 30520, 'text': 'a'}, {'startTimeMs': 30520, 'endTimeMs': 30720, 'text': 'lot'}, {'startTimeMs': 30720, 'endTimeMs': 30880, 'text': 'more'}, {'startTimeMs': 30880, 'endTimeMs': 31200, 'text': 'like'}, {'startTimeMs': 31640, 'endTimeMs': 31800, 'text': 'an'}, {'startTimeMs': 31800, 'endTimeMs': 32120, 'text': 'actual'}, {'startTimeMs': 32120, 'endTimeMs': 32400, 'text': 'human'}, {'startTimeMs': 32400, 'endTimeMs': 32880, 'text': 'voice.'}], 'locale': 'en-US'}, {'speaker': 'Speaker 4', 'startTimeMs': 33680, 'endTimeMs': 37200, 'text': 'Orlando ground 9555 requesting the end of pushback.', 'confidence': 0.823, 'words': [{'startTimeMs': 33680, 'endTimeMs': 34160, 'text': 'Orlando'}, {'startTimeMs': 34160, 'endTimeMs': 34600, 'text': 'ground'}, {'startTimeMs': 34600, 'endTimeMs': 35680, 'text': '9555'}, {'startTimeMs': 35680, 'endTimeMs': 36200, 'text': 'requesting'}, {'startTimeMs': 36200, 'endTimeMs': 36280, 'text': 'the'}, {'startTimeMs': 36320, 'endTimeMs': 36520, 'text': 'end'}, {'startTimeMs': 36520, 'endTimeMs': 36680, 'text': 'of'}, {'startTimeMs': 36680, 'endTimeMs': 37200, 'text': 'pushback.'}], 'locale': 'en-US'}, {'speaker': 'Speaker 4', 'startTimeMs': 38680, 'endTimeMs': 41280, 'text': '9555 request to end pushback received.', 'confidence': 0.823, 'words': [{'startTimeMs': 38680, 'endTimeMs': 39600, 'text': '9555'}, {'startTimeMs': 39600, 'endTimeMs': 40080, 'text': 'request'}, {'startTimeMs': 40080, 'endTimeMs': 40160, 'text': 'to'}, {'startTimeMs': 40280, 'endTimeMs': 40440, 'text': 'end'}, {'startTimeMs': 40480, 'endTimeMs': 40880, 'text': 'pushback'}, {'startTimeMs': 40880, 'endTimeMs': 41280, 'text': 'received.'}], 'locale': 'en-US'}], 'cameraShotTimesMs': [1467, 3233, 7367, 8200, 11367, 13567, 16100, 19433, 23967, 30033, 33200, 35267, 37700, 39200, 42033], 'segments': [{'startTimeMs': 726, 'endTimeMs': 1360, 'description': 'The video begins with a visual of a flight simulator and Microsoft Azure AI logo, indicating a collaboration between the two entities.', 'segmentId': '1'}, {'startTimeMs': 1360, 'endTimeMs': 13320, 'description': 'A speaker discusses the importance of good data for neural TTS, mentioning the creation of a universal TTS model based on 3,000 hours of data to capture audio nuances and generate natural voices.', 'segmentId': '2'}, {'startTimeMs': 13320, 'endTimeMs': 23680, 'description': 'The video shows visuals of data centers and flight simulations, highlighting the accumulation of data and the high fidelity of cognitive services offerings that sound more like human voices.', 'segmentId': '3'}, {'startTimeMs': 23680, 'endTimeMs': 41283, 'description': 'The video transitions to a demonstration of aircraft operations, including scenes of aircraft on the runway and communication between ground control and pilots.', 'segmentId': '4'}, {'startTimeMs': 41283, 'endTimeMs': 43230, 'description': 'The video concludes with the Microsoft logo, emphasizing the branding of the collaboration.', 'segmentId': '5'}]}]}, 'usage': {'videoHours': 0.013, 'tokens': {'contextualization': 12222.223, 'input': 10780, 'output': 482}}}\n"
     ]
    }
   ],
   "source": [
    "prebuilt_video_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-videoAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "video_url = \"https://github.com/kuljotSB/RAGwithAzureOpenAI/raw/refs/heads/main/ContentUnderstanding/Samples/FlightSimulator.mp4\"\n",
    "\n",
    "body = {\n",
    "    \"url\": video_url\n",
    "}\n",
    "\n",
    "video_analysis_result = {}\n",
    "\n",
    "try:\n",
    "    headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "            }\n",
    "\n",
    "    response = requests.post(prebuilt_video_analyzer_url, headers=headers, json=body)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    analysis_id = result.get(\"id\")\n",
    "    print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "    # Using the analysis ID to get results; polling until the analysis is complete\n",
    "    get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "    }\n",
    "    analysis_status = \"Running\"\n",
    "    while analysis_status == \"Running\":\n",
    "        status_response = requests.get(get_result_url, headers=headers)\n",
    "        status_response.raise_for_status()\n",
    "        status_result = status_response.json()\n",
    "        analysis_status = status_result.get(\"status\")\n",
    "        print(\"Current Analysis Status:\", analysis_status)\n",
    "        if analysis_status == \"Running\":\n",
    "            import time\n",
    "            time.sleep(3)  # Wait before polling again\n",
    "    result_response = requests.get(get_result_url, headers=headers)\n",
    "    result_response.raise_for_status()\n",
    "    video_analysis_result = result_response.json()\n",
    "    print(\"Video Analysis Result:\", video_analysis_result)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "\n",
    "# ---------- small console helpers ----------\n",
    "def _hr(char=\"─\", width=80): return char * width\n",
    "def _h(text: str, width=80): return f\"{_hr('=')} \\n  {text.strip()} \\n{_hr('=')}\"\n",
    "def _subh(text: str): return f\"\\n{text}\\n{_hr()}\"\n",
    "def _kv(k: str, v: Any, k_width=24):\n",
    "    if isinstance(v, (dict, list)):\n",
    "        v_str = json.dumps(v, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        v_str = \"\" if v is None else str(v)\n",
    "    return f\"{k:<{k_width}} : {v_str}\"\n",
    "def _wrap_block(text: str, width=100, indent=\"    \"):\n",
    "    if not text: return \"\"\n",
    "    return textwrap.indent(textwrap.fill(text, width=width), indent)\n",
    "\n",
    "# ---------- field formatting ----------\n",
    "def _extract_value_from_fieldobj(fobj: Dict[str, Any]) -> Tuple[str, Any]:\n",
    "    \"\"\"\n",
    "    From an Azure field object like {\"type\":\"string\",\"valueString\":\"...\"}\n",
    "    return (type, value) using the first present among value* keys.\n",
    "    \"\"\"\n",
    "    ftype = fobj.get(\"type\") or \"unknown\"\n",
    "    for key in (\"valueString\", \"valueNumber\", \"valueInteger\", \"valueBoolean\", \"valueArray\", \"valueObject\"):\n",
    "        if key in fobj:\n",
    "            return ftype, fobj[key]\n",
    "    return ftype, fobj\n",
    "\n",
    "def _fields_to_markdown(fields: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render 'fields' dict to Markdown bullets with types and values.\n",
    "    Complex values are JSON in fenced blocks. Handles arrays/objects.\n",
    "    \"\"\"\n",
    "    if not fields:\n",
    "        return \"_No fields._\"\n",
    "    lines = []\n",
    "    for fname, fval in fields.items():\n",
    "        if isinstance(fval, dict) and \"type\" in fval:\n",
    "            ftype, v = _extract_value_from_fieldobj(fval)\n",
    "        else:\n",
    "            ftype, v = \"unknown\", fval\n",
    "        if isinstance(v, (dict, list)):\n",
    "            v_md = \"```json\\n\" + json.dumps(v, indent=2, ensure_ascii=False) + \"\\n```\"\n",
    "        else:\n",
    "            v_md = \"\" if v is None else str(v)\n",
    "        lines.append(f\"- **{fname}** (_{ftype}_): {v_md}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- video/audio utilities ----------\n",
    "def _fmt_ms(ms: Optional[int]) -> str:\n",
    "    \"\"\"Format milliseconds as HH:MM:SS.mmm\"\"\"\n",
    "    if ms is None:\n",
    "        return \"\"\n",
    "    td = timedelta(milliseconds=int(ms))\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    seconds = total_seconds % 60\n",
    "    millis = int(ms) % 1000\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{millis:03d}\"\n",
    "\n",
    "def _phrases_to_markdown(phrases: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Render transcriptPhrases into a compact Markdown list with timestamps, speaker, confidence, and text.\n",
    "    \"\"\"\n",
    "    if not phrases:\n",
    "        return \"_No transcript phrases._\"\n",
    "    out = []\n",
    "    for p in phrases:\n",
    "        spk = p.get(\"speaker\") or \"Speaker\"\n",
    "        st = _fmt_ms(p.get(\"startTimeMs\"))\n",
    "        et = _fmt_ms(p.get(\"endTimeMs\"))\n",
    "        conf = p.get(\"confidence\")\n",
    "        txt = p.get(\"text\") or \"\"\n",
    "        conf_str = f\"{conf:.3f}\" if isinstance(conf, (int, float)) else (str(conf) if conf is not None else \"\")\n",
    "        out.append(f\"- `{st} → {et}` **{spk}** (_conf: {conf_str}_): {txt}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _segments_to_markdown(segments: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Render plain segment dicts like [{'startTimeMs':.., 'endTimeMs':.., 'description':.., 'segmentId':..}]\n",
    "    \"\"\"\n",
    "    if not segments:\n",
    "        return \"_No segments._\"\n",
    "    out = []\n",
    "    for s in segments:\n",
    "        st = _fmt_ms(s.get(\"startTimeMs\"))\n",
    "        et = _fmt_ms(s.get(\"endTimeMs\"))\n",
    "        desc = s.get(\"description\") or \"\"\n",
    "        sid = s.get(\"segmentId\")\n",
    "        head = f\"- **Segment {sid}**\" if sid else \"- **Segment**\"\n",
    "        out.append(f\"{head} `{st} → {et}`: {desc}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _times_to_markdown(times_ms: List[int], title: str) -> str:\n",
    "    if not times_ms:\n",
    "        return f\"_No {title.lower()}._\"\n",
    "    return \"\\n\".join(f\"- `{_fmt_ms(t)}`\" for t in times_ms)\n",
    "\n",
    "# ---------- main function ----------\n",
    "def display_video_analysis_result(\n",
    "    analysis_result: Dict[str, Any],\n",
    "    save_markdown_path: Optional[str] = None,\n",
    "    max_markdown_chars: int = 2000,\n",
    "    width: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-prints Azure 'prebuilt-videoAnalyzer' result and optionally writes a\n",
    "    Markdown report that includes analyzer markdown, Fields, segments, keyframes,\n",
    "    camera shots, and transcript phrases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    analysis_result : dict\n",
    "        The JSON-decoded response (\"Video Analysis Result\").\n",
    "    save_markdown_path : str | None\n",
    "        If provided, writes a Markdown report with analyzer markdown, fields, and extras.\n",
    "    max_markdown_chars : int\n",
    "        Truncate console preview of analyzer markdown to this many characters.\n",
    "    width : int\n",
    "        Wrap width for console output.\n",
    "    \"\"\"\n",
    "    # Console summary\n",
    "    print(_h(\"Content Understanding • Video Analysis Summary\", width))\n",
    "    print(_kv(\"Analysis ID\", analysis_result.get(\"id\")))\n",
    "    print(_kv(\"Status\", analysis_result.get(\"status\")))\n",
    "\n",
    "    result = (analysis_result or {}).get(\"result\", {})\n",
    "    usage = (analysis_result or {}).get(\"usage\", {}) or {}\n",
    "    tokens = usage.get(\"tokens\", {}) if isinstance(usage, dict) else {}\n",
    "    video_hours = usage.get(\"videoHours\")\n",
    "\n",
    "    print(_subh(\"Analyzer Info\"))\n",
    "    print(_kv(\"Analyzer ID\", result.get(\"analyzerId\")))\n",
    "    print(_kv(\"API Version\", result.get(\"apiVersion\")))\n",
    "    print(_kv(\"String Encoding\", result.get(\"stringEncoding\")))\n",
    "    created_at = result.get(\"createdAt\")\n",
    "    try:\n",
    "        created_at_local = (\n",
    "            datetime.fromisoformat(created_at.replace(\"Z\", \"+00:00\")).astimezone().isoformat()\n",
    "            if created_at else None\n",
    "        )\n",
    "    except Exception:\n",
    "        created_at_local = created_at\n",
    "    print(_kv(\"Created At (UTC)\", created_at))\n",
    "    print(_kv(\"Created At (local)\", created_at_local))\n",
    "    warnings = result.get(\"warnings\") or []\n",
    "    print(_kv(\"Warnings\", f\"{len(warnings)}\"))\n",
    "\n",
    "    print(_subh(\"Video Metadata\"))\n",
    "    print(_kv(\"Start Time\", _fmt_ms(result.get(\"startTimeMs\"))))\n",
    "    print(_kv(\"End Time\", _fmt_ms(result.get(\"endTimeMs\"))))\n",
    "    print(_kv(\"Width\", result.get(\"width\")))\n",
    "    print(_kv(\"Height\", result.get(\"height\")))\n",
    "\n",
    "    print(_subh(\"Usage\"))\n",
    "    if video_hours is not None:\n",
    "        print(_kv(\"Video Hours\", video_hours))\n",
    "    for k in (\"contextualization\", \"input\", \"output\"):\n",
    "        if k in tokens:\n",
    "            print(_kv(f\"Tokens.{k}\", tokens.get(k)))\n",
    "\n",
    "    contents = result.get(\"contents\") or []\n",
    "    print(_subh(f\"Contents ({len(contents)})\"))\n",
    "\n",
    "    # Prepare Markdown report parts (if requested)\n",
    "    md_parts = []\n",
    "    if save_markdown_path:\n",
    "        md_parts.append(\"# Video Analysis Report\\n\")\n",
    "        md_parts.append(\"## Summary\\n\")\n",
    "        md_parts.append(f\"- **Analysis ID:** `{analysis_result.get('id')}`\")\n",
    "        md_parts.append(f\"- **Status:** `{analysis_result.get('status')}`\")\n",
    "        md_parts.append(f\"- **Analyzer:** `{result.get('analyzerId')}`\")\n",
    "        md_parts.append(f\"- **API Version:** `{result.get('apiVersion')}`\")\n",
    "        md_parts.append(f\"- **String Encoding:** `{result.get('stringEncoding')}`\")\n",
    "        md_parts.append(f\"- **Created At (UTC):** `{created_at}`\")\n",
    "        md_parts.append(f\"- **Warnings:** `{len(warnings)}`\")\n",
    "        md_parts.append(f\"- **Video Window:** `{_fmt_ms(result.get('startTimeMs'))} → {_fmt_ms(result.get('endTimeMs'))}`\")\n",
    "        md_parts.append(f\"- **Resolution:** `{result.get('width')} x {result.get('height')}`\\n\")\n",
    "        if video_hours is not None:\n",
    "            md_parts.append(f\"- **Video Hours:** `{video_hours}`\")\n",
    "        if tokens:\n",
    "            md_parts.append(\"\\n## Usage\")\n",
    "            for k in (\"contextualization\", \"input\", \"output\"):\n",
    "                if k in tokens:\n",
    "                    md_parts.append(f\"- **Tokens.{k}:** `{tokens.get(k)}`\")\n",
    "        md_parts.append(\"\\n## Contents\\n\")\n",
    "\n",
    "    # Iterate content blocks\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        fields = (item.get(\"fields\") or {})\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "\n",
    "        # Console view\n",
    "        print(_hr())\n",
    "        print(f\"[Content #{idx}] kind={kind}\")\n",
    "\n",
    "        if fields:\n",
    "            print(\"• Fields:\")\n",
    "            for fname, fval in fields.items():\n",
    "                if isinstance(fval, dict):\n",
    "                    ftype, v = _extract_value_from_fieldobj(fval)\n",
    "                    if v is None: v = fval\n",
    "                    print(_wrap_block(f\"  - {fname} ({ftype}): {v}\", width))\n",
    "                else:\n",
    "                    print(_wrap_block(f\"  - {fname}: {fval}\", width))\n",
    "\n",
    "        preview = md.strip()\n",
    "        preview_trunc = (preview[:max_markdown_chars] + \" … [truncated]\") if len(preview) > max_markdown_chars else preview\n",
    "        print(\"• Analyzer Markdown Preview:\")\n",
    "        print(_wrap_block(preview_trunc, width))\n",
    "\n",
    "        # Markdown report for this content\n",
    "        if save_markdown_path:\n",
    "            md_parts.append(f\"### Content #{idx}\")\n",
    "            meta_bits = []\n",
    "            if kind: meta_bits.append(f\"**kind:** `{kind}`\")\n",
    "            if meta_bits:\n",
    "                md_parts.append(\"> \" + \" • \".join(meta_bits))\n",
    "\n",
    "            # Analyzer-provided markdown (exact)\n",
    "            md_parts.append(\"#### Analyzer Markdown\")\n",
    "            md_parts.append(md if md.strip() else \"_(empty)_\")\n",
    "\n",
    "            # Fields\n",
    "            md_parts.append(\"#### Fields\")\n",
    "            md_parts.append(_fields_to_markdown(fields))\n",
    "\n",
    "            md_parts.append(\"---\")\n",
    "\n",
    "    # Extras outside 'contents'\n",
    "    keyframe_times = result.get(\"KeyFrameTimesMs\") or []\n",
    "    camera_shots = result.get(\"cameraShotTimesMs\") or []\n",
    "    plain_segments = result.get(\"segments\") or []\n",
    "    transcript_phrases = result.get(\"transcriptPhrases\") or []\n",
    "\n",
    "    # Console extras\n",
    "    print(_subh(\"Key Frames\"))\n",
    "    if keyframe_times:\n",
    "        print(_wrap_block(_times_to_markdown(keyframe_times, \"Key Frames\"), width))\n",
    "    else:\n",
    "        print(\"_No key frames._\")\n",
    "    print(_subh(\"Camera Shots\"))\n",
    "    if camera_shots:\n",
    "        print(_wrap_block(_times_to_markdown(camera_shots, \"Camera Shots\"), width))\n",
    "    else:\n",
    "        print(\"_No camera shots._\")\n",
    "    print(_subh(\"Segments\"))\n",
    "    print(_wrap_block(_segments_to_markdown(plain_segments), width))\n",
    "    print(_subh(\"Transcript Phrases (compact)\"))\n",
    "    print(_wrap_block(_phrases_to_markdown(transcript_phrases), width))\n",
    "\n",
    "    # Markdown extras\n",
    "    if save_markdown_path:\n",
    "        md_parts.append(\"## Key Frames\")\n",
    "        md_parts.append(_times_to_markdown(keyframe_times, \"Key Frames\"))\n",
    "        md_parts.append(\"\\n## Camera Shots\")\n",
    "        md_parts.append(_times_to_markdown(camera_shots, \"Camera Shots\"))\n",
    "        md_parts.append(\"\\n## Segments\")\n",
    "        md_parts.append(_segments_to_markdown(plain_segments))\n",
    "        md_parts.append(\"\\n## Transcript Phrases\")\n",
    "        md_parts.append(_phrases_to_markdown(transcript_phrases))\n",
    "\n",
    "        out_path = Path(save_markdown_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        report = \"\\n\".join(md_parts).rstrip() + \"\\n\"\n",
    "        out_path.write_text(report, encoding=\"utf-8\")\n",
    "        print(_subh(\"Files\"))\n",
    "        print(_kv(\"Markdown saved\", str(out_path.resolve())))\n",
    "\n",
    "display_video_analysis_result(video_analysis_result, save_markdown_path=\"video_analysis.md\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
